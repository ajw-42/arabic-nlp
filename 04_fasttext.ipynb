{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-fasttext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo/VWjND5vjrZfW/PU32Y3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uek_2PM9NBM",
        "colab_type": "text"
      },
      "source": [
        "Previously, we trained a set of word embeddings and used them to improve our text classification model. But we ran into problems related to the size of our dataset. How might we overcome this problem? One solution is pre-trained word embeddings. These are word embedding models trained on a very large corpus -- say, all of Wikipedia. We can get the vectors out of this model in the same way that we did for our own smaller model previously. If there is some overlap between the domain of the training data and the domain of our data, we should get good results.\n",
        "\n",
        "One example of pre-trained word embeddings is the `fasttext` library, which provides a well-performing approach to train embeddings but also makes available pre-trained embeddings for 157 languages trained on Wikipedia and Common Crawl data: https://fasttext.cc/docs/en/crawl-vectors.html **The pre-trained Arabic embeddings that we use here take a while to download, and longer to upload to Google Drive, so you might prefer to run this notebook locally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SlWVXu79Jcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8xRng72APKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "7235aaba-3cc4-4133-8a08-2e00e2d8e7fc"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_train.csv')\n",
        "val = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_val.csv')\n",
        "test = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_test.csv')\n",
        "\n",
        "train.head()\n",
        "\n",
        "#Again, we'll limit ourselves to 20 classes for now.\n",
        "train = train[train.label <= 20]\n",
        "test = test[test.label <= 20]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQiDrU2pFypX",
        "colab_type": "text"
      },
      "source": [
        "Once we've downloaded the `cc.ar.300.vec` file, we load it in using `gensim`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX-3GVzlAPXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "\n",
        "path = 'gdrive/My Drive/cc.ar.300.bin'\n",
        "model = gensim.models.fasttext.FastText.load_fasttext_format(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzivNgUHAd2b",
        "colab_type": "text"
      },
      "source": [
        "We have access to the same word similarity utilities we used previously, so let's take a look at those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC8oaWXfAlm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fd5f1957-3d19-4a8a-ae41-53d9c617a0b3"
      },
      "source": [
        "model.similarity('الحرب', 'الصراع')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4434066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrw2qOKnAly8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ea037c8b-af86-4d0d-bc6c-98d1ea8bbb84"
      },
      "source": [
        "model.most_similar('المعارضة')[:5]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('والمعارضة', 0.7426829934120178),\n",
              " ('للمعارضة', 0.7048141360282898),\n",
              " ('بالمعارضة', 0.6725300550460815),\n",
              " ('فالمعارضة', 0.6373220682144165),\n",
              " ('معارضة', 0.6321747303009033)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUKQwD4oAmER",
        "colab_type": "text"
      },
      "source": [
        "Interesting! \"War\" and \"conflict\" are less similar here--perhaps because in this broader set of training data, the words are used in many different contexts--but for \"opposition\" we're getting exclusively variants on that word as most similar. So this model might have a better understanding of Arabic grammar than the small model we trained from scratch.\n",
        "\n",
        "Now we'll use the same function we built previously to get document level embeddings, and pass them into a logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI5advz_A8eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_vectorizer(text, model):\n",
        "  doc_vec = 0\n",
        "  count = 0\n",
        "\n",
        "  if len(text) == 1:\n",
        "    return model[text]\n",
        "\n",
        "  for t in text:\n",
        "    try:\n",
        "      word_vec = model[t]\n",
        "      doc_vec = doc_vec + word_vec\n",
        "      count += 1\n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "  doc_vec = doc_vec / count\n",
        "  return doc_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5UMYBcIJVu5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "76f4e8a3-de9f-42a4-961a-f457b779e1f0"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('arabic')\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "train_words = [tokenizer.tokenize(t) for t in train.text]\n",
        "test_words = [tokenizer.tokenize(t) for t in test.text]\n",
        "\n",
        "train_words = [[t for t in text if t not in stop_words] for text in train_words]\n",
        "test_words = [[t for t in text if t not in stop_words] for text in test_words]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aak-xFkRA8qo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "6ec95111-1394-4801-eaa0-950c0054312f"
      },
      "source": [
        "X_train = [doc_vectorizer(t, model) for t in train_words]\n",
        "X_test = [doc_vectorizer(t, model) for t in test_words]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "Y_train = train.label\n",
        "Y_test = test.label\n",
        "\n",
        "classifier = LogisticRegression(max_iter = 5000, multi_class='multinomial').fit(X_train, Y_train)\n",
        "preds = classifier.predict(X_test)\n",
        "\n",
        "f1_score(Y_test, preds, average = 'weighted')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5151575854701926"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enj8Pf2D1qVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a39ed2b-e089-493a-d4a0-99504a89b877"
      },
      "source": [
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "Y_train = train.label\n",
        "Y_test = test.label\n",
        "\n",
        "classifier = LinearSVC(max_iter = 5000, multi_class='ovr').fit(X_train, Y_train)\n",
        "preds = classifier.predict(X_test)\n",
        "\n",
        "f1_score(Y_test, preds, average = 'weighted')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7640660411073206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia8AMXoeA9Bi",
        "colab_type": "text"
      },
      "source": [
        "Using the logistic regression we've looked at previously, our f1 score is much worse! We can improve the score by using an SVC instead, but it's still around the same as it was previously. So we aren't getting a significant benefit from using these pre-trained embeddings.\n",
        "\n",
        "You might wonder if we could combine the knowledge contained in these pre-trained embeddings with the specific context of our dataset. What a convenient question! We sure can. This is called fine-tuning, and we'll get to it soon."
      ]
    }
  ]
}