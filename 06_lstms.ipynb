{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06-lstms.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3AXEQ2JESyQGEM5PrrtvR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6YyknHfT2mK",
        "colab_type": "text"
      },
      "source": [
        "We've looked at several options for text classification using word embeddings, so let's move on to deep learning! Deep learning is better suited to datasets larger than ours, but we'll still take a look at a few approaches and see how they perform.\n",
        "\n",
        "We'll also need to use a GPU, either locally if that's an option for you or through Google Colab. On Colab, go to Edit -> Notebook Settings and select GPU. Then run the cell below to ensure a GPU is available. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zfBncyiT1K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "    \n",
        "else:\n",
        "    print('No GPU available - maybe try again later?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wCQl1ujVfmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQEGcdnrVfxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "37bce877-5204-4abd-8c1b-cffe40db233c"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_train.csv')\n",
        "val = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_val.csv')\n",
        "test = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_test.csv')\n",
        "\n",
        "train.head()\n",
        "\n",
        "#On a GPU, we can use the full 40 classes! You could uncomment the two lines below if you're trying to run this on CPU.\n",
        "#train = train[train.label <= 20]\n",
        "#test = test[test.label <= 20]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>لوكسمبورغ: كاميرون ارتكب خطأ تاريخيا بطرح الاس...</td>\n",
              "      <td>استفتاء_بريطانيا</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>روسيا بصدد تصنيع مركبة فضائية جديدة\\n تبدأ عمل...</td>\n",
              "      <td>التقنية_والمعلومات</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>صادرات ألمانيا إلى روسيا عند أدنى مستوى  منذ 1...</td>\n",
              "      <td>عقوبات_اقتصادية</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>الجيش السوري يصد هجوم جبهة النصرة في ريف حلب\\n...</td>\n",
              "      <td>المعارضة_السورية</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ردود أفعال وسائل إعلام غربية على عملية درع الف...</td>\n",
              "      <td>الأزمة_السورية</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text            category  label\n",
              "0  لوكسمبورغ: كاميرون ارتكب خطأ تاريخيا بطرح الاس...    استفتاء_بريطانيا      1\n",
              "1  روسيا بصدد تصنيع مركبة فضائية جديدة\\n تبدأ عمل...  التقنية_والمعلومات     10\n",
              "2  صادرات ألمانيا إلى روسيا عند أدنى مستوى  منذ 1...     عقوبات_اقتصادية     25\n",
              "3  الجيش السوري يصد هجوم جبهة النصرة في ريف حلب\\n...    المعارضة_السورية     12\n",
              "4  ردود أفعال وسائل إعلام غربية على عملية درع الف...     الأزمة_السورية      6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1yrTX1T8WA",
        "colab_type": "text"
      },
      "source": [
        "We're going to build a basic LSTM (Long Short-Term Memory) network, which until a few years ago was state of the art for text classification tasks because they allow networks to retain information about pieces of text that aren't necessary close together in a document - hence the name. Here's a longer explanation: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
        "\n",
        "Let's try to implement this using `keras`, a high-level API for building basic neural networks. It's available through `tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5S1GO3-3URF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9a7cca60-937a-4043-8798-608c683f0dba"
      },
      "source": [
        "#As in previous notebooks, we'll tokenize our text and remove stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('arabic')\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "train_words = [tokenizer.tokenize(t) for t in train.text]\n",
        "val_words = [tokenizer.tokenize(t) for t in val.text]\n",
        "test_words = [tokenizer.tokenize(t) for t in test.text]\n",
        "\n",
        "train_words = [[t for t in text if t not in stop_words] for text in train_words]\n",
        "val_words = [[t for t in text if t not in stop_words] for text in val_words]\n",
        "test_words = [[t for t in text if t not in stop_words] for text in test_words]\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlrHF2JzW9TD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#num_words sets the size of our vocabulary - so we're keeping the 7000 most common words here\n",
        "tokenizer = Tokenizer(num_words = 8000, oov_token='[OOV]')\n",
        "tokenizer.fit_on_texts(train_words)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3ZGqjvP4OfX",
        "colab_type": "text"
      },
      "source": [
        "Now we have the words in our training set mapped to indices. Less common words in the training set, as well as any words that appear in `val` and/or `test` but not `train`, are out of vocabulary. As discussed in previous notebooks, we can't ask a model to evaluate words that it hasn't been trained on.\n",
        "\n",
        "We use our `word_index` to convert our words to indices and then \"pad\" each sequence of indices so that they're the same length. Sentences are different lengths, in other words, but tensorflow wants each input to have the same size. This is a standard step in deep learning for text, no matter what library or approach you're using. The `maxlen` argument specifies the maximum sentence length that we'll allow.\n",
        "\n",
        "This also converts our inputs to tensors (read more here, but in short a tensor is a multi-dimensional array), the format expected by tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBavLPOaW9dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_words)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=300, padding='post', truncating='post')\n",
        "\n",
        "validation_sequences = tokenizer.texts_to_sequences(val_words)\n",
        "val_padded = pad_sequences(validation_sequences, maxlen=300, padding='post', truncating='post')\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_words)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=300, padding='post', truncating='post')\n",
        "\n",
        "#Convert labels to arrays\n",
        "train_labels = np.array(train.label)\n",
        "val_labels = np.array(val.label)\n",
        "test_labels = np.array(test.label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3_yGFtXq6Y",
        "colab_type": "text"
      },
      "source": [
        "Now we can build our model! `keras` makes this super easy, which hides a lot of complexity but makes it a great entry going.\n",
        "\n",
        "We wrap our model in a call to `tf.keras.Sequential`, which lets us use a list to build the layers of our network.\n",
        "\n",
        "We start with an embedding layer, with our vocab size (8000) as an input and an embedding size (we've chosen 64 here) as an output.\n",
        "Then we have a single bidirectional LSTM layer and two linear layers.\n",
        "\n",
        "Our final linear layer has 40 output nodes, because we have 40 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ars79W9vXpdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "7de52937-5179-40f1-ed3a-13a7d04510d3"
      },
      "source": [
        "keras_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(8000, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(40, activation='softmax')\n",
        "])\n",
        "\n",
        "keras_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          512000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 40)                2600      \n",
            "=================================================================\n",
            "Total params: 588,904\n",
            "Trainable params: 588,904\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bix2-L4yW9Zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "1f6a12ff-48de-4e90-90ae-c095af108154"
      },
      "source": [
        "keras_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "num_epochs = 10\n",
        "\n",
        "history = keras_model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(val_padded, val_labels), verbose=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "692/692 - 21s - loss: 2.5847 - accuracy: 0.3034 - val_loss: 1.9898 - val_accuracy: 0.4243\n",
            "Epoch 2/10\n",
            "692/692 - 20s - loss: 1.7632 - accuracy: 0.4771 - val_loss: 1.7177 - val_accuracy: 0.4940\n",
            "Epoch 3/10\n",
            "692/692 - 20s - loss: 1.4150 - accuracy: 0.5657 - val_loss: 1.5697 - val_accuracy: 0.5352\n",
            "Epoch 4/10\n",
            "692/692 - 20s - loss: 1.1465 - accuracy: 0.6336 - val_loss: 1.4549 - val_accuracy: 0.5815\n",
            "Epoch 5/10\n",
            "692/692 - 20s - loss: 0.9883 - accuracy: 0.6771 - val_loss: 1.3958 - val_accuracy: 0.5905\n",
            "Epoch 6/10\n",
            "692/692 - 20s - loss: 0.8474 - accuracy: 0.7192 - val_loss: 1.4441 - val_accuracy: 0.5978\n",
            "Epoch 7/10\n",
            "692/692 - 20s - loss: 0.7508 - accuracy: 0.7457 - val_loss: 1.4861 - val_accuracy: 0.5894\n",
            "Epoch 8/10\n",
            "692/692 - 20s - loss: 0.6775 - accuracy: 0.7622 - val_loss: 1.5176 - val_accuracy: 0.5786\n",
            "Epoch 9/10\n",
            "692/692 - 21s - loss: 0.6176 - accuracy: 0.7800 - val_loss: 1.5874 - val_accuracy: 0.5779\n",
            "Epoch 10/10\n",
            "692/692 - 20s - loss: 0.5757 - accuracy: 0.7921 - val_loss: 1.6029 - val_accuracy: 0.5829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjmvv5TEX9Ub",
        "colab_type": "text"
      },
      "source": [
        "So what are we looking at here? For each epoch, or iteration over the dataset, we see the loss and accuracy for both our train and val sets. Loss--tracked here using categorical crossentropy, but different problems require different loss functions--is a measurement of how well the model is fitting to the training data. You might be familiar with the idea that for linear regression, you can look at the mean squared error. This is a loss function!\n",
        "\n",
        "So lower numbers are better for loss. Here we see that with each epoch, the training loss is going down and the accuracy is going up. Great.\n",
        "\n",
        "But in our last few epochs, we see that the validation loss is going up and the validation accuracy is going down, even though the training metrics continue to improve. This likely means that the model is overfitting - meaning that it's beginning to memorize characteristics of the training data specifically, rather than learning generalizable trends about the dataset. If we wanted to improve the model, we might train it for fewer epochs or use early stopping, a technique where we ask the model to stop training if it's beginning to overfit.\n",
        "\n",
        "Looking for overfitting is one important reason to use a validation set. It's considered best practice not to tweak your hyperparameters or otherwise changing your models based on test metrics. A val set lets you get a sense of how your model is doing, for instance on overfitting, while still holding out a 'real' test set. \n",
        "\n",
        "Speaking of, let's see how this model performs on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY_en7CbT7yn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "14d542a7-03a1-45f5-b34f-dcd5a65e4734"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "preds = keras_model.predict(test_padded)\n",
        "\n",
        "preds[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8.7735877e-08, 2.8430168e-07, 4.6861078e-11, 6.0553972e-11,\n",
              "       5.6843977e-11, 1.4128086e-08, 1.3539029e-06, 1.2695617e-10,\n",
              "       7.0358871e-19, 3.3114012e-11, 7.9905517e-14, 7.7171863e-12,\n",
              "       9.6451316e-08, 1.1430596e-06, 2.8013128e-13, 2.3505014e-07,\n",
              "       2.3402378e-12, 3.0051389e-05, 3.1096235e-05, 7.8456594e-07,\n",
              "       1.8648237e-10, 3.8474067e-08, 3.0096146e-04, 7.2220775e-12,\n",
              "       6.8299113e-14, 8.5205615e-10, 2.4046727e-07, 3.9145602e-15,\n",
              "       3.7505172e-09, 9.9962890e-01, 3.0067194e-12, 5.3953602e-07,\n",
              "       6.7811083e-13, 1.7839497e-07, 1.9960604e-09, 4.9507221e-12,\n",
              "       1.7385561e-20, 1.1451222e-09, 3.8347107e-06, 8.4892463e-08],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkC1gDJGEb-g",
        "colab_type": "text"
      },
      "source": [
        "This looks different from our previous predictions! Instead of a single predicted label, tensorflow gives us a probability that the text belongs to each label.\n",
        "\n",
        "This can be helpful in many ways, but an easy way to collapse this down to a single prediction is to get the label that our model thinks is most likely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOStN1eDERK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_flat_preds(preds):\n",
        "\n",
        "  max_preds = []\n",
        "\n",
        "  for pred in preds:\n",
        "    for i, x in enumerate(pred):\n",
        "      if x==np.max(pred):\n",
        "        max_preds.append(i)\n",
        "  \n",
        "  return max_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKMAbmzKA-jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0542b5cb-c759-4e6f-b40a-6c9dc99834e3"
      },
      "source": [
        "flat_preds = get_flat_preds(preds)\n",
        "\n",
        "f1_score(test.label, flat_preds, average = 'weighted')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5888271234276691"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkSU-Y00GhEk",
        "colab_type": "text"
      },
      "source": [
        "Not bad! This is worse than our previous results, but remember that we're working with 40 classes instead of 20 now. We could probably also improve our results by increasing hyperparameters such as the vocab size and sequence length, or by improving our model architecture. Right now, for instance, we don't have any [dropout] (https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DQ8bQ5t-u2M",
        "colab_type": "text"
      },
      "source": [
        "Because we're using a fairly small dataset here, at least by deep learning standards, we've been able to skip a few steps that are standard for a real deep learning problem. We'll see some of these in the final notebook on BERT, but a few examples are:\n",
        "\n",
        "- **Batching** - If it's too computationally expensive to pass our entire training set into a model, we might pass it in batches or subsets of the data. Both tensorflow and pytorch have utilities to make this easier.\n",
        "- **Selecting an activation function, an optimizer, a learning rate...** - There are many choices you can make when building a neural network, and it's probably a good idea to read up on the basics on, say, the difference between some common activation functions. In practice, however, you'll often replicate these choices from someone else who has solved a problem similar to yours!"
      ]
    }
  ]
}