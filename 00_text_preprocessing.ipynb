{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0-text-preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMYjeQDToCsaTIvhr6XRdy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX0uUJ8hsbvY",
        "colab_type": "text"
      },
      "source": [
        "Before we get started, we'll need to clean our dataset. We'll only do a bit of preprocessing here, since the different approaches we try have slightly different expectations in terms of preprocessing.\n",
        "\n",
        "I've downloaded the file RTAnews.OriginalTexts.zip from [https://data.mendeley.com/datasets/322pzsdxwy/1](https://data.mendeley.com/datasets/322pzsdxwy/1). It contains training and test folders, with news items for each category in seperate folders.\n",
        "\n",
        "We'd like to get one dataframe with the text and labels, then split it into training, validation, and test sets.\n",
        "\n",
        "You can rerun this notebook if you like, but the cleaned files are also availabe in this GitHub repo as csvs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NgECD7BsEJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjlsijXct9Al",
        "colab_type": "code",
        "outputId": "e2a973b3-eeed-49e0-9acd-b04d416ef815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "#mount Google Drive to access the raw version of the data.\n",
        "#if you're trying to run this notebook locally, just skip/delete this cell and change the path in the cell below\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/RTAnews_raw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vGAyZVUEYS0",
        "colab_type": "text"
      },
      "source": [
        "We'll load our data by creating empty dataframes, and then looping through the folders that contain text files for each category of news. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siK0VD8JsIxG",
        "colab_type": "code",
        "outputId": "6f139595-3488-4a95-cdfc-8a635f1283a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path = 'gdrive/My Drive/RTAnews_raw'\n",
        "\n",
        "for folder in os.listdir(os.path.join(path,'training')):\n",
        "    file_list = os.listdir(os.path.join(path,'training', folder))\n",
        "    for file_path in file_list:\n",
        "        with open(os.path.join(path,'training', folder, file_path)) as f_input:\n",
        "            temp = (f_input.read())\n",
        "            df = pd.DataFrame({'text':[temp]})\n",
        "            df['category'] = folder\n",
        "        train = train.append(df)\n",
        "print('training data loaded!')\n",
        "\n",
        "\n",
        "test = pd.DataFrame()\n",
        "\n",
        "for folder in os.listdir(os.path.join(path,'test')):\n",
        "    file_list = os.listdir(os.path.join(path,'test', folder))\n",
        "    for file_path in file_list:\n",
        "        with open(os.path.join(path,'test', folder, file_path)) as f_input:\n",
        "            temp = (f_input.read())\n",
        "            df = pd.DataFrame({'text':[temp]})\n",
        "            df['category'] = folder\n",
        "        test = test.append(df)\n",
        "print('test data loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data loaded!\n",
            "test data loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl09OH35v-KO",
        "colab_type": "code",
        "outputId": "a9e71873-b822-4fe8-9a4e-e1992fca1b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(f'Training data size:{len(train)}\\n',\n",
        "      f'Test data size:{len(test)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data size:16610\n",
            " Test data size:11060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iwVhVFhUH3o",
        "colab_type": "text"
      },
      "source": [
        "That's more test data than we need, so we'll create one dataframe and do our own split for both test and val sets.\n",
        "\n",
        "In addition to the category names, we'll want integer values for each category. `sklearn` has a `LabelEncoder` utility for exactly this purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3cavHlKUIJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "le = LabelEncoder().fit(df.category)\n",
        "df['labels'] = le.transform(df.category)\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "val,test = train_test_split(test, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bo4yL0Wvi4p",
        "colab_type": "text"
      },
      "source": [
        "So now we have 80% of our data in a training set, 10% in a validation set, and 10% in a test set. We'll save those files to csv, and we're done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN8oZ_DaviGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv(os.path.join(path,'arabic_train.csv'), index=False)\n",
        "val.to_csv(os.path.join(path,'arabic_val.csv'), index=False)\n",
        "test.to_csv(os.path.join(path,'arabic_test.csv'), index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}