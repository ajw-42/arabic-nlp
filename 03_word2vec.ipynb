{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZMiqyVDm3ZKn/ubBKg4PC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xEbaIyc89wu",
        "colab_type": "text"
      },
      "source": [
        "Approaches to representing text like `CountVectorizer` are sometimes called bag-of-words approaches, because they don't contain any information about the sequencing of the words. We're also using a fairly small dataset, so our model doesn't necessarily learn any general information about the mea\n",
        "\n",
        "The next few notebooks will introduce more advanced techniques that try to address this problem. First, we'll take a look at word embeddings by focusing on the implementation of Word2Vec available in the `gensim` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rWOc1_-85hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGMTHLUw-nzl",
        "colab_type": "code",
        "outputId": "9ae2543f-61d6-46b7-be54-758b5f3cd38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_train.csv')\n",
        "val = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_val.csv')\n",
        "test = pd.read_csv('gdrive/My Drive/RTANews_raw/arabic_test.csv')\n",
        "\n",
        "train.head()\n",
        "\n",
        "#Again, we'll limit ourselves to 20 classes for now.\n",
        "train = train[train.label <= 20]\n",
        "test = test[test.label <= 20]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmgj7jfc-ghh",
        "colab_type": "text"
      },
      "source": [
        "We need to add a new preprocessing step here - tokenizing our text into individual words. `nltk`, which we used in a previous notebook for loading stopwords, has a nice utility for this. We're using a naive tokenizer that just looks for spaces between words, but `nltk` offers more advanced approaches as well, for example making use of regular expressions.\n",
        "\n",
        "We'll also remove stopwords again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHfmMnZJGU-k",
        "colab_type": "code",
        "outputId": "1f67d3de-906a-4409-ae67-8fb147ad854a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('arabic')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wGEc7Lq0Dyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "train_words = [tokenizer.tokenize(t) for t in train.text]\n",
        "test_words = [tokenizer.tokenize(t) for t in test.text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc1VUMHpGORn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_words = [[t for t in text if t not in stop_words] for text in train_words]\n",
        "test_words = [[t for t in text if t not in stop_words] for text in test_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy5LgeoO1ffs",
        "colab_type": "text"
      },
      "source": [
        "Now we're going to train a set of word embeddings using the popular Word2Vec approach.\n",
        "\n",
        "Word embeddings are high-dimensional vector representations of words. In previous notebooks, we were also providing vector representations of our text, but using sparse vectors, or vectors where most values are zero (\"this word appears zero times in this document\") and therefore don't contain any information.\n",
        "\n",
        "Word2Vec is a specific approach for learning word embeddings with a shallow neural network. You can read more about it, and word embeddings in general, here: https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa.\n",
        "\n",
        "We pass a few arguments when creating our model, to define the minimum number of times a word can appear to be included in the model and to use the skigram implementation of word2vec (the other being cbow, or continous bag of words, which does better for datasets larger than ours)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S3wgaSG-naN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.Word2Vec(train_words, min_count=1, sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvlkVUiY_Czr",
        "colab_type": "text"
      },
      "source": [
        "Now we should be able to get the embedding for an indiviudal word out of our trained model. Let's give it a try:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68gbOuE6--Nm",
        "colab_type": "code",
        "outputId": "4e7bbaa8-b843-4672-a749-cd38c78c80f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "source": [
        "model['المعارضة']"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.14549054,  0.599306  , -0.4804341 ,  0.03285765, -0.5277679 ,\n",
              "        0.18579623, -0.3790516 , -0.16893376, -0.0919387 , -0.69887435,\n",
              "       -0.47992244,  0.5604452 ,  1.7088275 , -0.39851496, -0.93221676,\n",
              "        0.38124117, -0.34682694, -0.10315377,  0.34234753, -0.22387849,\n",
              "        0.36565235, -1.7127385 ,  0.3991669 ,  0.4639502 ,  0.16955788,\n",
              "       -0.08727596,  0.68151236, -0.533671  , -0.9149401 ,  0.6349782 ,\n",
              "        1.0968806 ,  0.09654019,  0.24191156, -0.11690563,  0.196531  ,\n",
              "        0.03272146, -0.0201999 , -0.09860993, -0.41145647, -1.1043358 ,\n",
              "        0.09548824,  0.30044132, -0.2053842 ,  0.1468742 ,  0.10847885,\n",
              "       -0.6022765 ,  0.90401655,  0.276518  ,  0.06019887, -0.8702403 ,\n",
              "       -0.33587086, -0.4497403 , -0.5549428 ,  0.4569512 ,  0.55942935,\n",
              "        0.39943692,  0.09723862, -0.0251064 ,  0.37965676, -0.16792785,\n",
              "       -0.26170108,  0.12032393, -0.30160433,  0.58695364, -0.5198097 ,\n",
              "       -0.32774565,  0.1265672 , -0.8171582 ,  0.8184151 , -0.03844127,\n",
              "        1.1451749 ,  0.00633759, -0.8307332 ,  0.20897952, -0.25715977,\n",
              "        0.7420546 , -0.14288247,  0.42651883,  0.7363889 , -0.17638245,\n",
              "        0.43681163,  0.3211297 ,  0.44055647, -0.6138141 ,  0.60782313,\n",
              "        0.18529056, -0.6590018 , -0.6769269 ,  0.10419076, -0.28963834,\n",
              "       -0.26678225,  0.14792207,  0.33289006, -0.30166423,  0.5134304 ,\n",
              "       -0.2969811 ,  0.430138  ,  0.16444892,  1.0944519 , -0.03145883],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGBJrQ-A_R9o",
        "colab_type": "text"
      },
      "source": [
        "A brief tangent: one appealing characteristic of word embeddings is that similar words should be near to each other in embedding space. `gensim` allows us to test this by computing the similarity between two words and by returning the most similar words to a given word.\n",
        "\n",
        "So let's give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iubDaaK_MVg",
        "colab_type": "code",
        "outputId": "d88b64a4-c782-43f5-aaaa-48f74a2f7373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "model.similarity('الحرب', 'الصراع')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6769255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8gNCkc3kmA",
        "colab_type": "text"
      },
      "source": [
        "So the words 'war' and 'conflict' are fairly similar - makes sense.\n",
        "\n",
        "Now, we'll look at the five words most similar to \"opposition.\" This is also built into the `gensim` implementation of word2vec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niXnsJ0iAFJP",
        "colab_type": "code",
        "outputId": "b48e6a59-45bf-4e58-80ce-1408e2c9702c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "model.most_similar('المعارضة')[:5]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('المعتدلة', 0.8478207588195801),\n",
              " ('للمعارضة', 0.8072715997695923),\n",
              " ('الفصائل', 0.7820077538490295),\n",
              " ('معارضة', 0.7800770401954651),\n",
              " ('المسلحة', 0.7489530444145203)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rONtRQoTANGy",
        "colab_type": "text"
      },
      "source": [
        "This looks decent! Two of these are variants on the word opposition and the others are \"armed,\" \"factions,\" and \"moderate\" -- all words that Arab media outlets might use to describe an opposition movement.\n",
        "\n",
        "We could further improve our results here by using a different metric, cosine similarity, to measure the distance between words in embedding space. But for now, let's return to our classification task.\n",
        "\n",
        "Right now, we have vectors for individual words, but we want a single vector for each piece of text in our dataset. One way to solve this problem is by averaging the vectors of individual words. Let's create a simple function that does this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ4-tSiFAMeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remember that we've already tokenized our text, creating a 'list of lists.' Let's plan to feed that into our function.\n",
        "def doc_vectorizer(text, model):\n",
        "  doc_vec = 0\n",
        "  count = 0\n",
        "\n",
        "  if len(text) == 1:\n",
        "    return model[text]\n",
        "\n",
        "  for t in text:\n",
        "    try:\n",
        "      word_vec = model[t]\n",
        "      doc_vec = doc_vec + word_vec\n",
        "      count += 1\n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "  doc_vec = doc_vec / count\n",
        "  return doc_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5phu3-J3u4A",
        "colab_type": "code",
        "outputId": "c4963853-7650-4ff1-84e0-9c8e1735506e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "X_train = [doc_vectorizer(t, model) for t in train_words]\n",
        "X_test = [doc_vectorizer(t, model) for t in test_words]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnr8x11vCcN3",
        "colab_type": "text"
      },
      "source": [
        "For similicity and for better comparison, let's feed our new vectors into the same `sklearn` classifier that we've used previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK6OfYWKCasO",
        "colab_type": "code",
        "outputId": "a5376480-359a-473b-a93c-4d26630609cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "Y_train = train.label\n",
        "Y_test = test.label\n",
        "\n",
        "classifier = LogisticRegression(max_iter = 5000, multi_class='multinomial').fit(X_train, Y_train)\n",
        "preds = classifier.predict(X_test)\n",
        "pd.crosstab(Y_test, preds)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>280</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>131</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0  0   1   2   3   4   5    6   7   8   ...  12  13  14  15  16   17  18  19  20\n",
              "label                                       ...                                     \n",
              "0      16   0   0   0   0   0    2   0   0  ...   0   0   0   0   4    1   0   0   0\n",
              "1       0  14   0   0   0   1    0   0   0  ...   0   2   0   0   0    0   0   0   0\n",
              "2       0   0  37   0   0   0    0   0   1  ...   0   0   2   0   0    2   0   0   0\n",
              "3       0   0   0  28   1   0    2   0   0  ...   0   0   0   0   0    0   0   0   3\n",
              "4       0   0   0   0  37   0    0   0   0  ...   0   0   0   0   0    0   0   0   2\n",
              "5       0   0   0   0   0  44    7   0   0  ...   0   1   0   0   4    0   0   1   1\n",
              "6       0   0   0   2   0   5  280   2   0  ...  31   7   0   0   5    0   0   2   0\n",
              "7       0   0   0   1   0   0    2  45   0  ...   0   0   0   0   0    0   0   3   1\n",
              "8       0   0   6   0   0   0    0   0  51  ...   0   0   5   0   0    0   0   0   0\n",
              "9       0   0   0   0   0   0    0   0   0  ...   0   0   0   0   0    2   0   1   0\n",
              "10      0   0   2   0   0   0    0   0   3  ...   0   0   0   0   4    1   0   0   1\n",
              "11      0   0   0   0   0   0    6   0   0  ...   0   0   1   1   1    0   0   1   1\n",
              "12      0   0   0   0   0   0   45   0   0  ...  51   0   0   0   1    0   0   2   0\n",
              "13      0   1   0   0   0   0    9   0   0  ...   0  45   0   0   1    0   0   1   2\n",
              "14      0   0   5   0   0   0    0   0  12  ...   0   0  11   0   1    0   0   0   1\n",
              "15      0   0   0   0   0   0    0   0   0  ...   0   1   0  18   0    0   0   0   1\n",
              "16      0   0   1   0   0   0   11   1   0  ...   0   0   0   0  42    0   0   0   1\n",
              "17      0   1   1   0   1   3    7   2   0  ...   0   0   0   0   2  131   0   0   1\n",
              "18      0   0   0   0   0   0    0   0   0  ...   0   0   0   0   0    0  29   0   0\n",
              "19      0   0   0   0   0   0    5   3   0  ...   0   0   0   0   1    0   0  32   2\n",
              "20      0   0   1   0   0   0    5   0   0  ...   0   0   1   0   1    0   0   4  38\n",
              "\n",
              "[21 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnwhf0Ik5-zY",
        "colab_type": "code",
        "outputId": "45896070-dbbd-4283-815a-ed173392f209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(Y_test, preds, average = 'weighted')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.780616644380058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zeax6kh20ay",
        "colab_type": "text"
      },
      "source": [
        "This looks good. Notice that the model is getting confused between categories 6 and 12 less often. The F1 score is also slightly improved.\n",
        "\n",
        "But wait! There's another approach we can try here. `gensim` also offers a doc2vec implementation, for cases like ours where we really want document-level and not a word-level vectors. We use a different function within `gensim` to tokenize our documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wGZWTmq4Nl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train.text)]\n",
        "doc_model = Doc2Vec(documents, min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4XkyQOT5ipc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = [doc_model.infer_vector(t) for t in train.text]\n",
        "X_test = [doc_model.infer_vector(t) for t in test.text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C04U-41P56P2",
        "colab_type": "code",
        "outputId": "2611851b-63df-47a2-cd8a-f90af673007a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "classifier = LogisticRegression(max_iter = 5000, multi_class='multinomial').fit(X_train, Y_train)\n",
        "preds = classifier.predict(X_test)\n",
        "f1_score(Y_test, preds, average = 'weighted')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.43112670116544183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dreWtCN8EtG",
        "colab_type": "text"
      },
      "source": [
        "These results are much worse! That's probably due to the size of our training set. With more data, doc2vec would likely outperform word2vec for this kind of problem.\n",
        "\n",
        "In the next notebook, we'll explore how to take advantage of large training sets when we don't have access to them ourselves."
      ]
    }
  ]
}